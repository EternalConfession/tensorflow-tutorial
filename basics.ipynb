{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "TensorFlow is an open source library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation on almost any platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Models for Deep Learning\n",
    "### Symbolic v.s. Imperative Programs\n",
    "If you are a python or c++ programmer, then you are already familiar with imperative programs. Imperative style programs conduct the computation as we run them. Most code you write in python is imperative, for example:\n",
    "\n",
    "    import numpy as np\n",
    "    a = np.ones(10)\n",
    "    b = np.ones(10) * 2\n",
    "    c = b * a\n",
    "\n",
    "Symbolic programs are different. The following lines are an equivalent symbolic style program that achieves the same goal:\n",
    "\n",
    "    A = Variable()\n",
    "    B = Constant()\n",
    "    C = B * A\n",
    "    # compiles the function\n",
    "    f = compile(C)\n",
    "    # run the function\n",
    "    c = f.run(A=np.ones(10), B=np.ones(10)*2)\n",
    "\n",
    "when C = B * A is executed, there is no actual computation happening. Instead, these operations generate a computation graph (symbolic graph) that represents the computation. Symbolic programs separates computation graph definition, compiling, and running step.\n",
    "\n",
    "Generally speaking, imperative programs are more flexible, while symblic programs are more efficient (graph optimizations, better garbage collections).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get familiar with the following basic tensorflow methods:\n",
    "\n",
    "tf.Constant()\n",
    "\n",
    "tf.Variable()\n",
    "\n",
    "tf.placeholder()\n",
    "\n",
    "tf.Session()\n",
    "\n",
    "### Now let's first implement 'C=B*A' in TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define C=B*A in a symbolic way\n",
    "A = tf.Variable(tf.ones([10]))\n",
    "B = tf.constant(np.ones(10)*2, tf.float32)\n",
    "C = tf.mul(A, B)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    # initialize variables\n",
    "    sess.run(init)\n",
    "    # run the graph and evaluate C\n",
    "    c = sess.run([C])\n",
    "    print 'c:', c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate ground truth 100 x, y data points in NumPy, y = 3.0 * x + 1.0\n",
    "# Regress for W and b that compute y_data = W * x_data + b\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = 3.0 * x_data + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define trainable variables\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define graph operations\n",
    "y = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss, L2\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define optimizer for training\n",
    "train_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the operation that initializes variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # initialization\n",
    "    sess.run(init)\n",
    "\n",
    "    # starting training\n",
    "    for step in range(100):\n",
    "        # run optimizer during training\n",
    "        _ = sess.run([train_optimizer])\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(step, sess.run(W), sess.run(b))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
